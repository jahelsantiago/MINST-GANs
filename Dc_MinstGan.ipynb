{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dc_MinstGan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLwSmZWWvkyiAk3jLm+7na",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad877d892cf3450ea29ea08f9cc7102b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6ea14710cb64685bd3fc56bb679570b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_355cd7c4094346dda266fa8c87e98e2a",
              "IPY_MODEL_450a93a213ef4874b10a2b5372d077a7"
            ]
          }
        },
        "d6ea14710cb64685bd3fc56bb679570b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "355cd7c4094346dda266fa8c87e98e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_266546d8d303442098ec617802145960",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 469,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b9ec0327f72489db2ab2e5bb43df56f"
          }
        },
        "450a93a213ef4874b10a2b5372d077a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cae5792271bc4d5b896e0169352b4f9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/469 [00:03&lt;04:50,  1.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc0f48887df24af2ba7c736637b8b9fe"
          }
        },
        "266546d8d303442098ec617802145960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b9ec0327f72489db2ab2e5bb43df56f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cae5792271bc4d5b896e0169352b4f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc0f48887df24af2ba7c736637b8b9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahelsantiago/MINST-GANs/blob/main/Dc_MinstGan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0lfOT5cOkN3"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJgodglNPcuT"
      },
      "source": [
        "## GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCv20bmAPeDC"
      },
      "source": [
        "class Generator(nn.Module):\r\n",
        "  def __init__(self, z_dim = 10, img_chanel = 1, hidden_dim = 64):\r\n",
        "    '''\r\n",
        "    Generator Class\r\n",
        "    Values:\r\n",
        "        z_dim (int): the dimension of the noise vector.\r\n",
        "        im_chan (int): the number of channels in the images, fitted for the dataset used.\r\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\r\n",
        "        hidden_dim (int): the inner dimension.\r\n",
        "    '''\r\n",
        "\r\n",
        "    super(Generator,self).__init__()\r\n",
        "    self.z_dim = z_dim\r\n",
        "    self.gen = nn.Sequential(\r\n",
        "        self.get_gen_blok(z_dim, hidden_dim*4),\r\n",
        "        self.get_gen_blok(hidden_dim*4, hidden_dim*2),\r\n",
        "        self.get_gen_blok(hidden_dim*2, hidden_dim),\r\n",
        "        self.get_gen_blok(hidden_dim, img_chanel, final_layer = True)                          \r\n",
        "    )\r\n",
        "\r\n",
        "  def get_gen_blok(self, input_dim,output_dim, kernel = 3, stride = 2, final_layer = False):\r\n",
        "    '''\r\n",
        "    Function to return a sequence of operations corresponding to a generator block of DCGAN, \r\n",
        "    corresponding to a transposed convolution, a batchnorm (except for in the last layer), and an activation.\r\n",
        "    Parameters:\r\n",
        "        input_channels: how many channels the input feature representation has\r\n",
        "        output_channels: how many channels the output feature representation should have\r\n",
        "        kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\r\n",
        "        stride: the stride of the convolution\r\n",
        "        final_layer: a boolean, true if it is the final layer and false otherwise \r\n",
        "                  (affects activation and batchnorm)\r\n",
        "    '''\r\n",
        "    if not final_layer:      \r\n",
        "      model = nn.Sequential(\r\n",
        "        nn.ConvTranspose2d(input_dim,output_dim,kernel,stride),\r\n",
        "        nn.BatchNorm2d(output_dim),\r\n",
        "        nn.ReLU()\r\n",
        "      )\r\n",
        "    else: #Final layer\r\n",
        "      model = nn.Sequential(\r\n",
        "        nn.ConvTranspose2d(input_dim,output_dim,kernel,stride),\r\n",
        "        nn.Tanh()\r\n",
        "      )\r\n",
        "    return model\r\n",
        "\r\n",
        "  def unsqueeze_noise(self, noise):\r\n",
        "      '''\r\n",
        "      Function for completing a forward pass of the generator: Given a noise tensor, \r\n",
        "      returns a copy of that noise with width and height = 1 and channels = z_dim.\r\n",
        "      Parameters:\r\n",
        "          noise: a noise tensor with dimensions (n_samples, z_dim)\r\n",
        "      '''\r\n",
        "      return noise.view(len(noise), self.z_dim, 1, 1)\r\n",
        "\r\n",
        "  def forward(self, noise):\r\n",
        "      '''\r\n",
        "      Function for completing a forward pass of the generator: Given a noise tensor, \r\n",
        "      returns generated images.\r\n",
        "      Parameters:\r\n",
        "          noise: a noise tensor with dimensions (n_samples, z_dim)\r\n",
        "      '''\r\n",
        "      x = self.unsqueeze_noise(noise)\r\n",
        "      return self.gen(x)\r\n",
        "\r\n",
        "\r\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\r\n",
        "    '''\r\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\r\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\r\n",
        "    Parameters:\r\n",
        "        n_samples: the number of samples to generate, a scalar\r\n",
        "        z_dim: the dimension of the noise vector, a scalar\r\n",
        "        device: the device type\r\n",
        "    '''\r\n",
        "    return torch.randn(n_samples, z_dim, device=device)\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5WC3VLjs1wp"
      },
      "source": [
        "## DISCRMINATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kddp78vFsgL3"
      },
      "source": [
        "class Discriminator(nn.Module):\r\n",
        "  def __init__(self, im_chanel = 1, hidden_dim = 16):\r\n",
        "    super(Discriminator, self).__init__()\r\n",
        "    '''\r\n",
        "    Discriminator Class\r\n",
        "    Values:\r\n",
        "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\r\n",
        "              (MNIST is black-and-white, so 1 channel is your default)\r\n",
        "    hidden_dim: the inner dimension, a scalar\r\n",
        "    '''\r\n",
        "    \r\n",
        "    self.disc = nn.Sequential(\r\n",
        "        self.get_disc_block(im_chanel, hidden_dim),\r\n",
        "        self.get_disc_block(hidden_dim, hidden_dim*2),\r\n",
        "        self.get_disc_block(hidden_dim * 2, 1, final_layer = True),\r\n",
        "        \r\n",
        "    )\r\n",
        "  \r\n",
        "  def get_disc_block(self, inpud_dim, output_dim, kernel = 4, stride = 2, final_layer = False):\r\n",
        "    '''\r\n",
        "    Function to return a sequence of operations corresponding to a discriminator block of DCGAN, \r\n",
        "    corresponding to a convolution, a batchnorm (except for in the last layer), and an activation.\r\n",
        "    Parameters:\r\n",
        "      inpud_dim: how many channels the input feature representation has\r\n",
        "      output_dim: how many channels the output feature representation should have\r\n",
        "      kernel: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\r\n",
        "      stride: the stride of the convolution\r\n",
        "      final_layer: a boolean, true if it is the final layer and false otherwise \r\n",
        "                (affects activation and batchnorm)\r\n",
        "    '''\r\n",
        "    if not final_layer:\r\n",
        "      return nn.Sequential(\r\n",
        "          nn.Conv2d(inpud_dim, output_dim, kernel, stride),\r\n",
        "          nn.BatchNorm2d(output_dim),\r\n",
        "          nn.LeakyReLU(negative_slope=0.2)\r\n",
        "      )\r\n",
        "    else:\r\n",
        "      return nn.Sequential(\r\n",
        "          nn.Conv2d(inpud_dim, output_dim, kernel, stride)          \r\n",
        "      )\r\n",
        "\r\n",
        "  def forward(self,image):\r\n",
        "    '''\r\n",
        "    Function for completing a forward pass of the discriminator: Given an image tensor, \r\n",
        "    returns a 1-dimension tensor representing fake/real.\r\n",
        "    Parameters:\r\n",
        "        image: a flattened image tensor with dimension (im_dim)\r\n",
        "    '''\r\n",
        "    disc_pred = self.disc(image)\r\n",
        "    return disc_pred.view(len(disc_pred), -1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGTlDxrG1lhC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaSipj5_0tvI"
      },
      "source": [
        "from torchvision import transforms\r\n",
        "from torchvision.datasets import MNIST\r\n",
        "from torchvision.utils import make_grid\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "z_dim = 64\r\n",
        "display_step = 500\r\n",
        "batch_size = 128\r\n",
        "# A learning rate of 0.0002 works well on DCGAN\r\n",
        "lr = 0.0002\r\n",
        "\r\n",
        "# These parameters control the optimizer's momentum, which you can read more about here:\r\n",
        "# https://distill.pub/2017/momentum/ but you don’t need to worry about it for this course!\r\n",
        "beta_1 = 0.5 \r\n",
        "beta_2 = 0.999\r\n",
        "device = 'cpu'\r\n",
        "\r\n",
        "# You can tranform the image values to be between -1 and 1 (the range of the tanh activation)\r\n",
        "transform = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.5,), (0.5,)),\r\n",
        "])\r\n",
        "\r\n",
        "dataloader = DataLoader(\r\n",
        "    MNIST('.', download=True, transform=transform),\r\n",
        "    batch_size=batch_size,\r\n",
        "    shuffle=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xrXGvFm15ya"
      },
      "source": [
        "gen = Generator(z_dim).to(device)\r\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\r\n",
        "disc = Discriminator().to(device) \r\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\r\n",
        "\r\n",
        "# You initialize the weights to the normal distribution\r\n",
        "# with mean 0 and standard deviation 0.02\r\n",
        "def weights_init(m):\r\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\r\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "    if isinstance(m, nn.BatchNorm2d):\r\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\r\n",
        "        torch.nn.init.constant_(m.bias, 0)\r\n",
        "gen = gen.apply(weights_init)\r\n",
        "disc = disc.apply(weights_init)\r\n",
        "\r\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\r\n",
        "    '''\r\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\r\n",
        "    size per image, plots and prints the images in an uniform grid.\r\n",
        "    '''\r\n",
        "    image_tensor = (image_tensor + 1) / 2\r\n",
        "    image_unflat = image_tensor.detach().cpu()\r\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\r\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
        "    plt.show()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "ad877d892cf3450ea29ea08f9cc7102b",
            "d6ea14710cb64685bd3fc56bb679570b",
            "355cd7c4094346dda266fa8c87e98e2a",
            "450a93a213ef4874b10a2b5372d077a7",
            "266546d8d303442098ec617802145960",
            "3b9ec0327f72489db2ab2e5bb43df56f",
            "cae5792271bc4d5b896e0169352b4f9c",
            "fc0f48887df24af2ba7c736637b8b9fe"
          ]
        },
        "id": "XP7V_0kn2X89",
        "outputId": "4056763b-45ca-49ad-a981-8991e18ca735"
      },
      "source": [
        "from tqdm.auto import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "torch.manual_seed(0) # Set for testing purposes, please do not change!\r\n",
        "\r\n",
        "\r\n",
        "n_epochs = 50\r\n",
        "cur_step = 0\r\n",
        "mean_generator_loss = 0\r\n",
        "mean_discriminator_loss = 0\r\n",
        "for epoch in range(n_epochs):\r\n",
        "    # Dataloader returns the batches\r\n",
        "    for real, _ in tqdm(dataloader):\r\n",
        "        cur_batch_size = len(real)\r\n",
        "        real = real.to(device)\r\n",
        "\r\n",
        "        ## Update discriminator ##\r\n",
        "        disc_opt.zero_grad()\r\n",
        "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\r\n",
        "        fake = gen(fake_noise)\r\n",
        "        disc_fake_pred = disc(fake.detach())\r\n",
        "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\r\n",
        "        disc_real_pred = disc(real)\r\n",
        "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\r\n",
        "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\r\n",
        "\r\n",
        "        # Keep track of the average discriminator loss\r\n",
        "        mean_discriminator_loss += disc_loss.item() / display_step\r\n",
        "        # Update gradients\r\n",
        "        disc_loss.backward(retain_graph=True)\r\n",
        "        # Update optimizer\r\n",
        "        disc_opt.step()\r\n",
        "\r\n",
        "        ## Update generator ##\r\n",
        "        gen_opt.zero_grad()\r\n",
        "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\r\n",
        "        fake_2 = gen(fake_noise_2)\r\n",
        "        disc_fake_pred = disc(fake_2)\r\n",
        "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\r\n",
        "        gen_loss.backward()\r\n",
        "        gen_opt.step()\r\n",
        "\r\n",
        "        # Keep track of the average generator loss\r\n",
        "        mean_generator_loss += gen_loss.item() / display_step\r\n",
        "\r\n",
        "        ## Visualization code ##\r\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\r\n",
        "            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\r\n",
        "            show_tensor_images(fake)\r\n",
        "            show_tensor_images(real)\r\n",
        "            mean_generator_loss = 0\r\n",
        "            mean_discriminator_loss = 0\r\n",
        "        cur_step += 1\r\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad877d892cf3450ea29ea08f9cc7102b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-fb6f826b9cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mgen_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mfake_noise_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mfake_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_noise_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mdisc_fake_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_fake_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_fake_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-643893073b14>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     61\u001b[0m       '''\n\u001b[1;32m     62\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    927\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    928\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WunVtMYa2YTJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}